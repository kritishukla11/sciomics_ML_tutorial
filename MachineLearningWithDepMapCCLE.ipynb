{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f713a6a-345b-4068-b699-26820e130466",
   "metadata": {},
   "source": [
    "# Machine Learning with DepMap/CCLE Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7cfa97-341f-46e4-96e5-8f9191061763",
   "metadata": {},
   "source": [
    "In this tutorial, you will try to predict the effects of mutations on transcriptional subnetworks. Transcription factors are proteins that turn on the transcription of genes. Some transcription factors can regulate 1000s of genes. These genes then code for proteins that interact with other proteins. All of these genes and proteins make up a transcriptional subnetwork. \n",
    "\n",
    "In this tutorial, we will be looking at the protein KEAP1 and its relationship with the NRF2 transcriptional subnetwork. You will create a machine learning algorithm that will pick the KEAP1 mutations that are most likely to have effects on the NRF2 transcriptional subnetwork. The mutation data will be from DepMap/CCLE, and we will get transcriptional subnetwork information from DepMap/CCLE and MSigDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1aaf1-d465-427e-9e2b-a1919b39db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all packages\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import py3Dmol\n",
    "\n",
    "# Clustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from kneed import KneeLocator, DataGenerator as dg\n",
    "\n",
    "# Supervised Machine learning\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import xgboost as xg\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    cross_val_predict\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_curve, \n",
    "    roc_auc_score, \n",
    "    RocCurveDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb44dd-4167-4bcb-bd35-e62e21319f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas to show all columns of dataframes\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebd8fb-80de-46a4-abf5-79ca7577f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your working directory to your current path\n",
    "wd = '/work/users/k/r/kritis/chemical_education/final_notebook_ML'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a081d06-ed4b-4370-b595-54b406ab1a0b",
   "metadata": {},
   "source": [
    "## Importing Data from DepMap/CCLE and adding 3D coordinates.\n",
    "\n",
    "First, let's import the data from DepMap/CCLE. Then, we will map the 3D coordinates of each mutation into this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df55bd4-ea65-436e-a111-aaaaea3b5111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DepMap/CCLE mutation data\n",
    "depmap = pd.read_csv(f'{wd}/data/OmicsSomaticMutations.csv') #use f strings to create path\n",
    "depmap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a87c3-deb7-40dd-944c-908b1b334ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 3D coordinate info for all proteins\n",
    "coord = pd.read_csv(f'{wd}/data/3Dcoord_allgenes.csv') #use f strings to create path\n",
    "coord.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a502d-5e51-4667-a632-b13c47225ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep the DepMap/CCLE dataframe for mapping\n",
    "\n",
    "# First, let's filter to only include SNVs\n",
    "depmap = depmap[depmap['VariantType'] == 'SNV']\n",
    "depmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902840e7-4c4c-4280-bc31-8b353c66fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's use regex to extract the residue number from the ProteinChange column\n",
    "depmap['res'] = depmap['ProteinChange'].str.extract(r'^p\\.[A-Z](\\d+)')\n",
    "\n",
    "# Convert to numeric (will leave NaN for non-matching patterns)\n",
    "depmap['res'] = pd.to_numeric(depmap['res'], errors=\"coerce\")\n",
    "depmap = depmap.dropna(subset='res')\n",
    "\n",
    "depmap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73667539-2eb4-4c69-8cc4-87de667cc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data mapping by merging on common keys\n",
    "\n",
    "# Rename HugoSymbol to gene\n",
    "depmap = depmap.rename(columns={'HugoSymbol': 'gene'})\n",
    "\n",
    "# Merge on ['gene', 'res']\n",
    "merged = depmap.merge(coord, on=['gene', 'res'], how=\"inner\")\n",
    "\n",
    "# Drop rows where x, y, or z is NaN\n",
    "merged = merged.dropna(subset=[\"x_coord\", \"y_coord\", \"z_coord\"])\n",
    "\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af84f8-c8d6-4b73-9975-4992ac7bc57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract only the columns we need for Machine Learning\n",
    "ml_df = merged[['gene','res','x_coord','y_coord','z_coord','ModelID']].copy()\n",
    "\n",
    "ml_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832bad10-38b8-4660-bfb1-2ba87248f7cd",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Clustering is a type of unsupervised machine learning that groups data points based on their similarity without relying on labeled outcomes. Its goal is to identify natural patterns or structures in the data, such as clusters of similar observations, which can reveal underlying relationships or group behaviors. \n",
    "\n",
    "Among clustering methods, density-based clustering defines clusters as regions of high point density separated by areas of low density. This approach is particularly effective for identifying clusters of arbitrary shape and for distinguishing noise or outliers, as points in sparse regions are treated as noise rather than being forced into a cluster. \n",
    "\n",
    "Here, we will be using DBscan to create clustered regions of variants from our 3D protein data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2784653-4f28-4d3b-9d9a-865dd6005e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset dataframe to only KEAP1\n",
    "keap1 = ml_df[ml_df['gene']=='KEAP1'].reset_index(drop=True)\n",
    "keap1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f548bb-5051-424c-88f5-afceb0256927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with only the xyz coordinates, remove any duplicate coordinates\n",
    "xyz = keap1[['x_coord','y_coord','z_coord']].copy().drop_duplicates()\n",
    "xyz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30806dab-009e-40f7-b89a-9dcbc876bf97",
   "metadata": {},
   "source": [
    "A k-distance plot is a tool used in density-based clustering, particularly with DBSCAN, to help select the epsilon parameter. For each point in the dataset, the distance to its k-th nearest neighbor (commonly k = MinPts) is computed and then sorted in ascending order. When plotted, the curve typically shows a gradual increase followed by a sharp “knee” or inflection point. This point indicates a threshold distance that separates dense regions (clusters) from sparse regions (noise). Choosing epsilon near this knee allows DBSCAN to capture dense clusters while treating points in low-density regions as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d05bdb-4dad-4712-910c-abf7ce2bd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the k distance plot\n",
    "\n",
    "# Fit NearestNeighbors model\n",
    "n_neighbors = 6 #MinPts is 2 x dimension. Here, 2 x 3 = 6\n",
    "nbrs = NearestNeighbors(n_neighbors= n_neighbors).fit(xyz) \n",
    "\n",
    "# Compute distances and indices of neighbors\n",
    "neigh_dist, neigh_ind = nbrs.kneighbors(xyz)\n",
    "\n",
    "# Sort distances for each neighbor column-wise\n",
    "sorted_distances = np.sort(neigh_dist, axis=0)\n",
    "\n",
    "# Select the distance to the k-th nearest neighbor for each point\n",
    "k_distances = sorted_distances[:, n_neighbors - 2]\n",
    "\n",
    "# Convert to list (for plotting)\n",
    "k_dist_list = k_distances.tolist()\n",
    "\n",
    "# Prepare x-axis indices\n",
    "indices = list(range(len(k_dist_list)))\n",
    "\n",
    "# Plot k-distance plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(indices, k_dist_list, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Points sorted by k-distance\")\n",
    "plt.ylabel(f\"Distance to {n_neighbors-1}th nearest neighbor\")\n",
    "plt.title(f\"{n_neighbors-1}-Distance Plot (for DBSCAN eps selection)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c734d70-40a3-459a-9b13-9f2dcd861787",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "We can use this plot to select our epsilon parameter. Below, you will see how to use this epsilon parameter to perform density-based scanning for epsilon = 10. On your own, try epsilon = 10-14 and decide which you think is the best for the data based on the output clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae4838-35f7-45ce-9306-6cbf9932e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps= 10, min_samples=6).fit(xyz)\n",
    "cluster_list = clusters.labels_.tolist()\n",
    "xyz.insert(0, 'cluster', cluster_list)\n",
    "xyz['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a7eee-8111-4c1d-b42c-48bac73f99d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3d96aeb-a984-4e50-ad43-401ee80a1baf",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click here to reveal the answer</summary>\n",
    "\n",
    "  If you picked epsilon = 13, you are correct! This epsilon value is near the \"knee\" of our plot, and creates a variety of clusters in our protein, which is what we are looking for.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e1d57-c867-454a-967d-5fc308a40091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the clustering info back into the original KEAP1 dataframe\n",
    "keap1_clustered = keap1.merge(xyz, on=['x_coord', 'y_coord','z_coord'], how=\"inner\") \n",
    "keap1_clustered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127878a-d56f-4f44-a711-ae886a44eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster radius of each cluster\n",
    "\n",
    "# compute centroids\n",
    "centroids = keap1_clustered.groupby(\"cluster\")[[\"x_coord\",\"y_coord\",\"z_coord\"]].mean()\n",
    "\n",
    "# join centroids back\n",
    "df = keap1_clustered.join(centroids, on=\"cluster\", rsuffix=\"_centroid\")\n",
    "\n",
    "# compute distance from centroid\n",
    "df[\"dist_to_centroid\"] = np.sqrt(\n",
    "    (df[\"x_coord\"] - df[\"x_coord_centroid\"])**2 +\n",
    "    (df[\"y_coord\"] - df[\"y_coord_centroid\"])**2 +\n",
    "    (df[\"z_coord\"] - df[\"z_coord_centroid\"])**2\n",
    ")\n",
    "\n",
    "# get cluster radius = max distance\n",
    "radii = df.groupby(\"cluster\")[\"dist_to_centroid\"].max().rename(\"cluster_radius\")\n",
    "\n",
    "# merge back\n",
    "keap1_clustered = df.merge(radii, on=\"cluster\", how=\"left\")\n",
    "keap1_clustered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d55a41-218b-4acc-a8ea-c8eac8d9fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the density of each cluster\n",
    "\n",
    "# count points per cluster\n",
    "counts = keap1_clustered['cluster'].value_counts().rename(\"n_points\")\n",
    "\n",
    "# get cluster radius \n",
    "radii = keap1_clustered.groupby(\"cluster\")[\"cluster_radius\"].first()\n",
    "\n",
    "# compute volume of sphere\n",
    "volume = (4/3) * np.pi * (radii ** 3)\n",
    "\n",
    "# density = number of points / volume\n",
    "density = (counts / volume).rename(\"density\")\n",
    "\n",
    "# merge densities back into the dataframe\n",
    "cluster_props = pd.concat([counts, radii, volume.rename(\"volume\"), density], axis=1)\n",
    "keap1_clustered = keap1_clustered.merge(cluster_props[\"density\"], on=\"cluster\", how=\"left\")\n",
    "keap1_clustered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb64b55a-f544-4b36-a466-7baa21dec0a0",
   "metadata": {},
   "source": [
    "## Adding Ground Truth Data \n",
    "\n",
    "Now we’ll add our “Ground Truth” data. In supervised machine learning, we need a training dataset that contains examples with known outcomes. The algorithm learns patterns from the relationships between the input features and these known outcomes, so it can make accurate predictions on new, unseen data, which is our test dataset. Essentially, the ground truth serves as a reference that guides the model in understanding what “correct” predictions look like.\n",
    "\n",
    "Here, our ground truth data is in the form of Gene Set Enrichment Analysis (GSEA) scores. Each score tells us how much the NRF2 transcriptional subnetwork is active in cell lines that contain our mutations of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e38224-2864-42b7-ba26-d6e4d5561826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the GSEA data for NRF2\n",
    "gsea = pd.read_csv(f'{wd}/data/NRF2_GSEA_scores.csv')\n",
    "gsea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1713d2-43d1-4e7c-b281-fe3194a37855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping based on common Model ID/DepMap ID column\n",
    "keap1_ml = pd.merge(\n",
    "    keap1_clustered,\n",
    "    gsea,\n",
    "    left_on=\"ModelID\",\n",
    "    right_on=\"DepMap_ID\",\n",
    "    how=\"inner\"  # use 'left' or 'outer' if you want different merge behavior\n",
    ")\n",
    "keap1_ml = keap1_ml.drop(columns=[\"DepMap_ID\"]) # drop unneeded column\n",
    "keap1_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0683cd-ebd6-4609-802c-38ee0eb3af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the GSEA scores to make data processing easier\n",
    "keap1_ml['SINGH_NFE2L2_TARGETS_norm'] = (keap1_ml['SINGH_NFE2L2_TARGETS'] - keap1_ml['SINGH_NFE2L2_TARGETS'].mean()) / keap1_ml['SINGH_NFE2L2_TARGETS'].std()\n",
    "keap1_ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f98c3-b514-499e-b06c-192c045996c4",
   "metadata": {},
   "source": [
    "For this algorithm, we will be doing a binary classification. Variants will be either \"class1\" (associated with changes in the NRF2 transcriptional subnetwork) or \"class0\" (not associated). For our training data, we will take any cell lines with GSEA scores that are greater than one standard deviation above the mean (approx top 15%), and assign variants in those cell lines as \"class1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ffb5e-7f54-4191-91a6-22cb2081876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and std of the column\n",
    "mean_val = keap1_ml['SINGH_NFE2L2_TARGETS_norm'].mean()\n",
    "std_val = keap1_ml['SINGH_NFE2L2_TARGETS_norm'].std()\n",
    "\n",
    "# Create the 'class' column: 1 if value > mean + 1*std, else 0\n",
    "keap1_ml['class'] = (keap1_ml['SINGH_NFE2L2_TARGETS_norm'] > (mean_val + std_val)).astype(int)\n",
    "keap1_ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ab922-d2b0-4280-af37-588a5aa8380b",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning Algorithm\n",
    "\n",
    "Now it is time to work on our main machine learning algorithm. \n",
    "\n",
    "First, we will look at our class sizes. We want to see how many class0 and class1 variants are in our data. If one group is substantially bigger than the other, this data is considered \"unbalanced.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbc965-313c-435e-b0bb-e926fcbc6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "keap1_ml['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596c11c-2a26-4af2-a833-b8101d69f116",
   "metadata": {},
   "source": [
    "Clearly, our data is unbalanced. There are two popular stratgies for balancing data: random oversampling and random undersampling. Random oversampling balances classes by duplicating existing samples from the minority class until it matches the majority size. Random undersampling balances classes by randomly removing samples from the majority class until it matches the minority size.\n",
    "\n",
    "We don't want to get rid of any of our data, so here we will only practice balancing by random oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6d963-aac5-4829-9c5c-651bc7923ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = keap1_ml.drop(columns=['class'])\n",
    "y = keap1_ml['class']\n",
    "\n",
    "# Apply random oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Combine back into a dataframe\n",
    "keap1_ml_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns),\n",
    "                                pd.DataFrame(y_resampled, columns=['class'])],\n",
    "                               axis=1)\n",
    "\n",
    "keap1_ml_resampled['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4cb1cd-82e1-4a1f-81ea-afac13ab6670",
   "metadata": {},
   "source": [
    "Now, we will get into the machine learning algorithm. Here, you will see how to use the algorithm on the unbalanced data. Then, you will try it on the resampled data on your own. \n",
    "\n",
    "We are going to try two different types of classical machine learning algorithms: Random Forest and XGBoost.\n",
    "\n",
    "A Random Forest is an ensemble method that builds many decision trees on bootstrapped samples of the data and averages their predictions (or takes a majority vote) to reduce overfitting and improve generalization. Each tree is trained on a random subset of features, which increases diversity and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da68f2-b275-42d9-a958-04808dcf2280",
   "metadata": {},
   "source": [
    "In machine learning, features are the individual measurable properties or attributes of the data that the model uses to make predictions. Each feature represents one dimension of the input. The choice and quality of features strongly influence the model’s accuracy and interpretability. Our features here have to do with 3D properties of the variants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b53da-08f0-443c-9df3-a9c6d14b7a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate our features from our target prediction column\n",
    "X = keap1_ml[['x_coord', 'y_coord', 'z_coord','cluster','cluster_radius','density']]\n",
    "y = keap1_ml['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76233ada-7443-43b6-8553-dcbfda67ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Random Forest model and fit it\n",
    "rf_r = RandomForestClassifier(\n",
    "    n_estimators=300,       # Number of decision trees in the forest\n",
    "    random_state=42,        # Ensures reproducible results by fixing the random seed\n",
    "    class_weight='balanced' # Automatically adjusts weights to handle imbalanced classes\n",
    ")\n",
    "\n",
    "rf_r.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf929a-a162-4cf0-b97b-e572a0344be2",
   "metadata": {},
   "source": [
    "Cross-validation is a technique to evaluate a machine learning model’s performance by splitting the data into multiple folds, training the model on some folds, and testing it on the remaining fold. This process is repeated for all folds, providing a more reliable estimate of the model’s generalization ability than a single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c6f30-7f13-4648-92f4-872921561324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model\n",
    "y_probs = cross_val_predict(rf_r, X, y, cv=5, method='predict_proba')[:, 1]\n",
    "threshold = 0.55\n",
    "y_pred_adjusted = (y_probs >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb48ba2-f75b-4c6a-ae47-f65c6e7c8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the predictions to the dataframe\n",
    "keap1_ml['rf_pred_adjusted'] = y_pred_adjusted\n",
    "keap1_ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c1c7b-37ff-4f0a-80e7-b6c3a3dd76ee",
   "metadata": {},
   "source": [
    "Now, we will compute some different metrics to evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc085b4-2e25-4e9a-b3c4-79bfe2fb4880",
   "metadata": {},
   "source": [
    "| Metric | Description | Formula | Notes |\n",
    "|--------|-------------|---------|-------|\n",
    "| **Accuracy** | Overall fraction of correctly predicted samples | (TP + TN) / (TP + TN + FP + FN) | Works well for balanced datasets, but can be misleading if classes are imbalanced |\n",
    "| **Precision** | Fraction of predicted positives that are actually positive | TP / (TP + FP) | High precision → few false positives |\n",
    "| **Recall** | Fraction of actual positives correctly identified | TP / (TP + FN) | High recall → few false negatives; important for rare events or disease detection |\n",
    "| **F1 Score** | Harmonic mean of precision and recall | 2 * (Precision * Recall) / (Precision + Recall) | Balances precision and recall; useful for imbalanced datasets |\n",
    "| **MCC (Matthews Correlation Coefficient)** | Quality of binary classification accounting for all outcomes | (TP*TN - FP*FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN)) | Ranges from -1 (wrong) to 1 (perfect); 0 = random; robust for imbalanced datasets |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8363302b-24ce-475d-b2ce-0272d2324513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the scores\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred_adjusted))\n",
    "print(\"Precision:\", precision_score(y, y_pred_adjusted))\n",
    "print(\"Recall:\", recall_score(y, y_pred_adjusted))\n",
    "print(\"F1 Score:\", f1_score(y, y_pred_adjusted))\n",
    "print(\"MCC:\", matthews_corrcoef(y, y_pred_adjusted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3431e0-c864-46aa-93ed-c6ba12e474e4",
   "metadata": {},
   "source": [
    "Our scores don't look too good! Let's try looking at the ROC/AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f218be4-e4b4-4ac3-a1b0-f2edcbb8647b",
   "metadata": {},
   "source": [
    "The ROC AUC (Receiver Operating Characteristic – Area Under the Curve) measures a classifier’s ability to distinguish between classes across all possible decision thresholds. The ROC curve plots the true positive rate against the false positive rate, showing the trade-off between sensitivity and specificity. The AUC quantifies the overall performance: 1.0 means perfect classification, 0.5 means random guessing, and higher values indicate better discriminative ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb636f-d42a-4cf0-a4ab-d5d922ea687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for class 1\n",
    "y_probs = rf_r.predict_proba(X)[:, 1]  # probability of class 1\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y, y_probs)\n",
    "\n",
    "# Compute ROC AUC score\n",
    "roc_auc = roc_auc_score(y, y_probs)\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "\n",
    "# 4. Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # diagonal line for random chance\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994df32d-724e-4d0b-837a-358af1d430c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do this on your own for the resampled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92a31e-60b7-4c4d-8325-84e11a347548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51789fa2-1cbc-405f-ace2-15f420f8c6e4",
   "metadata": {},
   "source": [
    "You'll notice that the scores and ROC/AUC is much better for the resampled data. Now, let's try using the XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc16d35-e102-48c1-a60d-b015205f8a09",
   "metadata": {},
   "source": [
    "XGBoost (Extreme Gradient Boosting) is a high-performance machine learning algorithm based on gradient boosting of decision trees. It builds trees sequentially, where each new tree tries to correct the errors of the previous trees by minimizing a loss function. XGBoost adds regularization, weighted trees, and efficient handling of missing data, making it faster and often more accurate than traditional gradient boosting. It’s widely used in competitions and real-world applications for classification, regression, and ranking tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e502324-820b-4fb5-a014-92ba5bfb465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate our features from our target prediction column\n",
    "X = keap1_ml[['x_coord', 'y_coord', 'z_coord','cluster','cluster_radius','density']]\n",
    "y = keap1_ml['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8faca-6fea-4f57-8b74-99b163898293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost classifier\n",
    "xgb_clf = xg.XGBClassifier(\n",
    "    n_estimators=300,        # Number of trees\n",
    "    max_depth=5,             # Maximum tree depth\n",
    "    learning_rate=0.1,       # Step size shrinkage\n",
    "    scale_pos_weight=(y==0).sum() / (y==1).sum(),  # Handle class imbalance\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'    # Avoid warning in recent XGBoost versions\n",
    ")\n",
    "\n",
    "# Fit the model on the full dataset (or cross-validation for evaluation)\n",
    "xgb_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4a73d-fe8a-4879-bc97-263e8227df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model\n",
    "y_probs = cross_val_predict(xgb_clf, X, y, cv=5, method='predict_proba')[:, 1]\n",
    "threshold = 0.55\n",
    "y_pred_adjusted = (y_probs >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b47f4-e59a-46e4-a9a5-90abc850675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to dataframe\n",
    "keap1_ml['xgb_pred_adjusted'] = y_pred_adjusted\n",
    "keap1_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e19aaf-f941-439c-bc01-baacc88fe716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred_adjusted))\n",
    "print(\"Precision:\", precision_score(y, y_pred_adjusted))\n",
    "print(\"Recall:\", recall_score(y, y_pred_adjusted))\n",
    "print(\"F1 Score:\", f1_score(y, y_pred_adjusted))\n",
    "print(\"MCC:\", matthews_corrcoef(y, y_pred_adjusted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c04df40-e339-4f70-8e79-3a32c79737da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC/AUC\n",
    "\n",
    "# Get predicted probabilities for class 1 from XGBoost\n",
    "y_probs = xgb_clf.predict_proba(X)[:, 1]  # probability of class 1\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y, y_probs)\n",
    "\n",
    "# Compute ROC AUC score\n",
    "roc_auc = roc_auc_score(y, y_probs)\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # diagonal line for random chance\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5beb921-173b-4ad1-b3e9-00b0b1b9ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try it on your own for the resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6d5c9-0052-47d3-9933-92af83178e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcf0eeba-b32a-432e-af1b-34b7aec3fd8e",
   "metadata": {},
   "source": [
    "## Visualizing the Results\n",
    "\n",
    "We can see that our metrics look the best for the resampled data with the XGBoost model. Let's visualize the clusters picked out by this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a080271-512c-423f-b785-a552884768d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum class 1 counts per cluster\n",
    "class1_counts = keap1_ml_resampled.groupby('cluster')['xgb_pred_adjusted'].sum()\n",
    "\n",
    "# Find the cluster with the maximum number of class 1\n",
    "max_cluster = class1_counts.idxmax()\n",
    "max_count = class1_counts.max()\n",
    "\n",
    "print(f\"Cluster with most class 1: {max_cluster} (count = {max_count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7a2cf-098e-4b47-b6a6-82decca95876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter for cluster 0 and class 1\n",
    "cluster0_class1_variants = keap1_ml_resampled[(keap1_ml_resampled['cluster'] == 0) & \n",
    "                                              (keap1_ml_resampled['xgb_pred_adjusted'] == 1)]\n",
    "\n",
    "# Display the result\n",
    "cluster0_class1_variants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572c936-f390-4d66-9529-982ad3b598f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDB file (AlphaFold or PDB)\n",
    "with open(f\"{wd}/data/AF-Q14145-F1-model_v3.pdb\", \"r\") as f:\n",
    "    pdb_data = f.read()\n",
    "\n",
    "# Initialize 3D viewer\n",
    "view = py3Dmol.view(width=800, height=600)\n",
    "view.addModel(pdb_data, 'pdb')\n",
    "view.setStyle({'cartoon': {'color':'lightblue'}})  # main protein in light blue\n",
    "\n",
    "# List of residues to highlight in red\n",
    "red_residues = cluster0_class1_variants['res'].tolist()  # cluster 0 class 1 residue positions\n",
    "\n",
    "# Highlight each residue in red\n",
    "for res in red_residues:\n",
    "    view.addStyle({'resi': res}, {'stick': {'color': 'red'}})\n",
    "\n",
    "# Focus on the protein and show\n",
    "view.zoomTo()\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce015d40-8f29-4071-adbe-68ad730068a1",
   "metadata": {},
   "source": [
    "We can see that all the residues highlighted are in the KEAP1/NRF2 interaction site. It makes a lot of sense that mutations here would cause changes in the NRF2 transcriptional subnetwork! This shows how we can use Machine Learning to find biologically relevant patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519057e-1c05-487d-9a78-d9d94b4f04bc",
   "metadata": {},
   "source": [
    "**Optional: On your own, try adjusting things like the threshold, or the features used to see how that affects the scores.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89925d59-1185-4988-bd10-b0fcd809e964",
   "metadata": {},
   "source": [
    "This workflow was developed by Kriti Shukla at the University of North Carolina at Chapel Hill. If you are interested in learning more about how similar work can be applied in scientific contexts, please see the following paper: \n",
    "\n",
    "Shukla, K; Idanwekhai, K; Naradikian, M; Ting, S; Schoenberger, S; Brunk, E. Machine Learning of Three-Dimensional Protein Structures to Predict the Functional Impacts of Genome Variation. J. Chem. Inf. Model. 2024, 64, 13, 5328–5343"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
